# Indian Sign Language (ISL) Gesture Recognition â€“ LSTM + MediaPipe Hands

This project implements a **real-time Indian Sign Language (ISL) gesture recognition system** using:

- **MediaPipe Hands** for extracting 3D hand landmarks  
- **LSTM deep learning model** for gesture sequence classification  
- **OpenCV** for real-time webcam-based inference  

The system supports collecting gesture data, training an LSTM model, and running live gesture detection.

---

## ğŸš€ Features

### âœ” Data Collection  
- Capture hand gesture sequences using your webcam  
- Extract 21Ã—3 hand landmarks for up to **2 hands** (126 features per frame)  
- Save each gesture as `.npy` sequences  
- 30 frames per sequence and 30 samples per gesture  

### âœ” LSTM Model Training  
- Train on collected `.npy` sequences  
- Uses a 2-layer LSTM architecture  
- Automatically splits data into train/test sets  
- Saves:  
  - Trained model â†’ `isl_lstm_model.keras`  
  - Label file â†’ `labels.npy`

### âœ” Real-Time Gesture Detection  
- Detects hand landmarks from webcam  
- Maintains a rolling sequence of 50 frames  
- Predicts gesture using the trained LSTM model  
- Displays gesture name + confidence %

---

## ğŸ“ Project Structure
â”œâ”€â”€ dataset/
â”‚ â”œâ”€â”€ gesture1/
â”‚ â”œâ”€â”€ gesture2/
â”‚ â””â”€â”€ ...
â”œâ”€â”€ data_collection.py
â”œâ”€â”€ train_model.py
â”œâ”€â”€ gesture_detection.py
â”œâ”€â”€ labels.txt
â””â”€â”€ README.md


